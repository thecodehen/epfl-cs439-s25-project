# Warmup + AdamW + CosineAnnealing
optimizer:
  name: adamw_baseline  # same as folder name
  learning_rate: 1.e-3  # AdamW parameter 
  beta1: 0.9            # AdamW parameter
  beta2: 0.999          # AdamW parameter
  weight_decay: 0.01    # AdamW parameter
  epsilon: 1.e-8        # AdamW parameter
  # for lr scheduler parameters see `lr_schedulers/default.yaml`
