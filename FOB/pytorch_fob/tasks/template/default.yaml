task:
  name: template  # important for download directory and other things
  output_dir_name: template
  batch_size: 10
  max_epochs: 15
  max_steps: null  # The maximum number of optimizer steps to train the model. Is automatically calculated from epochs but can be manually set.
  target_metric: val_loss  # choose a metric that is being logged
  target_metric_mode: min  # min or max 
  dataset_split_lengths: [80, 10, 10]
  model:  # small MLP (name of model as comment might be a good idea)
    hidden_channels: 10
    activation: relu  # this parameter has to be in {relu, gelu}, modify the model.py to support more
engine:
  devices: 1
  sbatch_args:
    time: 00:05:00
optimizer:  # you can set your favourite baseline as default like this
  name: adamw_baseline
  learning_rate: 1.e-3  # you can also set a good default for the hyperparameter
evaluation:
  plot:
    metric: test_loss
    test_metric_mode: min
    format: "2.1"
    limits: [0, 100]
