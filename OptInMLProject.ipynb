{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "mT80DB4gUcOU",
        "vBNpfflWPIdL",
        "VwRfBjhEORc_"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Custom Optimizer SGD in Pytorch"
      ],
      "metadata": {
        "id": "mT80DB4gUcOU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3WZZWMBaDOWn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.optim import Optimizer\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomOptimizer(Optimizer):\n",
        "    def __init__(self, params, lr=1e-3):\n",
        "        # Define default hyperparameters\n",
        "        defaults = dict(lr=lr)\n",
        "        super(CustomOptimizer, self).__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        # Optional closure for reevaluating the model (e.g., for line search)\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            with torch.enable_grad():\n",
        "                loss = closure()\n",
        "\n",
        "        # Iterate over parameter groups\n",
        "        for group in self.param_groups:\n",
        "            lr = group['lr']\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                # Simple gradient descent step\n",
        "                p.add_(p.grad, alpha=-lr)\n",
        "                #p.data = p.data - lr * p.grad\n",
        "        return loss"
      ],
      "metadata": {
        "id": "dBhhkt1GU7Nc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TwoLayerNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(TwoLayerNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)   # First linear layer\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)  # Second linear layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))        # Activation after first layer\n",
        "        x = self.fc2(x)                # Second linear layer\n",
        "        return x     # Softmax for class probabilities"
      ],
      "metadata": {
        "id": "_6tbmofFZug6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TwoLayerNet(5, 2, 1)\n",
        "optimizer = CustomOptimizer(model.parameters(), lr=0.01)\n",
        "criterion = torch.nn.MSELoss()"
      ],
      "metadata": {
        "id": "OInW1CLKy8Lf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(32, 5)\n",
        "y = torch.randn(32, 1)\n",
        "\n",
        "print(list(model.parameters()), '\\n')\n",
        "\n",
        "# Training step\n",
        "output = model(x)\n",
        "loss = criterion(output, y)\n",
        "\n",
        "optimizer.zero_grad()\n",
        "loss.backward()"
      ],
      "metadata": {
        "id": "04wcWsamUx4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "before = model.fc1.weight.clone()\n",
        "optimizer.step()\n",
        "after = model.fc1.weight\n",
        "print(before, after)"
      ],
      "metadata": {
        "id": "S5xc6tely5R2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(optimizer.param_groups))\n",
        "print()\n",
        "print(optimizer.param_groups[0])\n",
        "print()\n",
        "print(optimizer.param_groups[0].keys())\n",
        "print()\n",
        "print(optimizer.param_groups[0]['lr'])\n",
        "print()\n",
        "print(optimizer.param_groups[0]['params'])\n",
        "print()\n",
        "print(len(optimizer.param_groups[0]['params']))\n",
        "print()\n",
        "print(optimizer.param_groups[0]['params'][0])\n",
        "print(optimizer.param_groups[0]['params'][1])\n",
        "print()\n",
        "print(optimizer.param_groups[0]['params'][0].grad)\n",
        "print(optimizer.param_groups[0]['params'][1].grad)"
      ],
      "metadata": {
        "id": "2Y6OiIumWeUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom Adam Optimizer"
      ],
      "metadata": {
        "id": "vBNpfflWPIdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.optim import Optimizer\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "axtO2cTcPKNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomAdam(Optimizer):\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8):\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(f\"Invalid lr: {lr}\")\n",
        "        if not 0.0 <= eps:\n",
        "            raise ValueError(f\"Invalid eps: {eps}\")\n",
        "        if not 0.0 <= betas[0] < 1.0:\n",
        "            raise ValueError(f\"Invalid beta1: {betas[0]}\")\n",
        "        if not 0.0 <= betas[1] < 1.0:\n",
        "            raise ValueError(f\"Invalid beta2: {betas[1]}\")\n",
        "\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps)\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs a single optimization step.\"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            with torch.enable_grad():\n",
        "                loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            lr, (beta1, beta2), eps = group['lr'], group['betas'], group['eps']\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad\n",
        "\n",
        "                # State initialization\n",
        "                state = self.state[p]\n",
        "                if not state:\n",
        "                    state['step'] = 0\n",
        "                    # Exponential moving average of gradient values\n",
        "                    state['exp_avg'] = torch.zeros_like(p)\n",
        "                    # Exponential moving average of squared gradient values\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p)\n",
        "\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "                state['step'] += 1\n",
        "                t = state['step']\n",
        "\n",
        "                # Update biased first moment estimate\n",
        "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
        "                # Update biased second raw moment estimate\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
        "\n",
        "                # Compute bias-corrected moments\n",
        "                bias_correction1 = 1 - beta1 ** t\n",
        "                bias_correction2 = 1 - beta2 ** t\n",
        "                corrected_avg = exp_avg / bias_correction1\n",
        "                corrected_avg_sq = exp_avg_sq / bias_correction2\n",
        "\n",
        "                # Parameter update\n",
        "                denom = corrected_avg_sq.sqrt().add_(eps)\n",
        "                p.addcdiv_(corrected_avg, denom, value=-lr)\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "1fjf2Bk3QckS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TwoLayerNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(TwoLayerNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)   # First linear layer\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)  # Second linear layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))        # Activation after first layer\n",
        "        x = self.fc2(x)                # Second linear layer\n",
        "        return x     # Softmax for class probabilities"
      ],
      "metadata": {
        "id": "NQBZIGEVSIUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TwoLayerNet(5, 2, 1)\n",
        "optimizer = CustomAdam(model.parameters(), lr=5e-4, betas=(0.9, 0.999), eps=1e-8)\n",
        "criterion = torch.nn.MSELoss()"
      ],
      "metadata": {
        "id": "PfJFYqyWSThq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(32, 5)\n",
        "y = torch.randn(32, 1)\n",
        "\n",
        "print(list(model.parameters()), '\\n')\n",
        "\n",
        "# Training step\n",
        "output = model(x)\n",
        "loss = criterion(output, y)\n",
        "\n",
        "optimizer.zero_grad()\n",
        "loss.backward()"
      ],
      "metadata": {
        "id": "mBZW8Pk3SdHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "before = model.fc1.weight.clone()\n",
        "optimizer.step()\n",
        "after = model.fc1.weight\n",
        "print(before, after)"
      ],
      "metadata": {
        "id": "o8yoc_JXSeCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(optimizer.param_groups))\n",
        "print()\n",
        "print(optimizer.param_groups[0])\n",
        "print()\n",
        "print(optimizer.param_groups[0].keys())\n",
        "print()\n",
        "print(optimizer.param_groups[0]['lr'])\n",
        "print()\n",
        "print(optimizer.param_groups[0]['params'])\n",
        "print()\n",
        "print(len(optimizer.param_groups[0]['params']))\n",
        "print()\n",
        "print(optimizer.param_groups[0]['params'][0])\n",
        "print(optimizer.param_groups[0]['params'][1])\n",
        "print()\n",
        "print(optimizer.param_groups[0]['params'][0].grad)\n",
        "print(optimizer.param_groups[0]['params'][1].grad)"
      ],
      "metadata": {
        "id": "P5b0ZYu3SkHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Flora Adam"
      ],
      "metadata": {
        "id": "VwRfBjhEORc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import contextlib\n",
        "import logging\n",
        "import math\n",
        "from functools import partial\n",
        "from typing import Any, Callable, Dict, Generator, Iterable, Optional, Sequence, Union\n",
        "import torch\n",
        "from torch.optim.optimizer import Optimizer\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "xiRJ34DHOUHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stable_randn(\n",
        "    shape: Union[int, Sequence[int]],\n",
        "    seed: int,\n",
        "    device: Optional[Union[str, torch.device]] = None,\n",
        "    dtype: Optional[torch.dtype] = torch.float32,\n",
        ") -> torch.Tensor:\n",
        "    if device is None:\n",
        "        device = torch.device(\"cpu\")\n",
        "    generator = torch.Generator(device=device).manual_seed(seed)\n",
        "    rn = torch.randn(shape, generator=generator, device=generator.device, dtype=dtype)\n",
        "    return rn"
      ],
      "metadata": {
        "id": "sAL1_rSOZYou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def next_seed(seed: int, adv: int = 0xF) -> int:\n",
        "    \"\"\"\n",
        "    This is a naive helper function to generate a new seed from the given seed.\n",
        "    \"\"\"\n",
        "    generator = torch.Generator().manual_seed(seed)\n",
        "    return torch.randint(\n",
        "        0, torch.iinfo(torch.int64).max, (adv,), generator=generator, device=generator.device\n",
        "    ).tolist()[-1]"
      ],
      "metadata": {
        "id": "pWJ99MiYPS2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_seed(seed: int) -> tuple[int, int]:\n",
        "    generator = torch.Generator().manual_seed(seed)\n",
        "    return tuple(\n",
        "        torch.randint(0, torch.iinfo(torch.int64).max, (2,), generator=generator, device=generator.device).tolist()\n",
        "    )"
      ],
      "metadata": {
        "id": "2MTEkt9DHF0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FloraAdam(Optimizer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        params: Union[Iterable[torch.Tensor], Iterable[Dict[str, Any]]],\n",
        "        lr:float = None,\n",
        "        betas: tuple[float, float] = (0.9, 0.999),\n",
        "        eps: float = 1e-8,\n",
        "        rank: int = None,\n",
        "        kappa: int = 1000,\n",
        "        seed: int = 0,\n",
        "    ) -> None:\n",
        "\n",
        "        defaults = {\n",
        "            \"lr\": lr,\n",
        "            \"betas\": betas,\n",
        "            \"eps\": eps,\n",
        "            \"rank\": rank,\n",
        "            \"kappa\": kappa,\n",
        "\n",
        "        }\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "        params_idx = seed\n",
        "        for group in self.param_groups:\n",
        "            for p in group[\"params\"]:\n",
        "                params_idx += 1\n",
        "                if p.requires_grad:\n",
        "                    self.state[p][\"seed\"] = params_idx\n",
        "\n",
        "    @staticmethod\n",
        "    def _should_compress(param_group: Dict, param_shape: tuple[int, ...]) -> tuple[bool, bool, bool]:\n",
        "        factored = len(param_shape) == 2\n",
        "        should_compress= (\n",
        "            param_group[\"rank\"] is not None\n",
        "            and param_group[\"rank\"] > 0\n",
        "            and factored\n",
        "            and min(param_shape) >= param_group[\"rank\"]\n",
        "            and max(param_shape) / min(param_shape) <= 4  # rule out embeddings\n",
        "        )\n",
        "        return should_compress\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs a single optimization step.\"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            with torch.enable_grad():\n",
        "                loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            lr, (beta1, beta2), eps = group['lr'], group['betas'], group['eps']\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad\n",
        "                grad_shape = grad.shape\n",
        "                should_compress = optimizer._should_compress(group, grad_shape)\n",
        "\n",
        "                print('\\n', 'grad_shape =', grad_shape)\n",
        "                print('should_compress =', should_compress)\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                # State initialization\n",
        "                if (state and'step' not in state) or (not state):\n",
        "                    state['step'] = 0\n",
        "\n",
        "                    if should_compress:\n",
        "                        if grad_shape[0] < grad_shape[-1]:\n",
        "                            cshape = (group[\"rank\"], grad_shape[-1])\n",
        "                        else:\n",
        "                            cshape = (grad_shape[0], group[\"rank\"])\n",
        "\n",
        "                        # Exponential moving average of gradient values\n",
        "                        state['exp_avg'] = torch.zeros(cshape).to(grad)\n",
        "                        # Exponential moving average of squared gradient values\n",
        "                        state['exp_avg_sq'] = torch.zeros(cshape).to(grad)\n",
        "\n",
        "                    else:\n",
        "                        # Exponential moving average of gradient values\n",
        "                        state['exp_avg'] = torch.zeros_like(p)\n",
        "                        # Exponential moving average of squared gradient values\n",
        "                        state['exp_avg_sq'] = torch.zeros_like(p)\n",
        "\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "                state['step'] += 1\n",
        "                t = state['step']\n",
        "\n",
        "                if should_compress:\n",
        "                    def _down_proj(seed, rank, tensor):\n",
        "                        lseed, rseed = split_seed(seed)\n",
        "                        if tensor.shape[0] < tensor.shape[-1]:\n",
        "                            left_projection = stable_randn(\n",
        "                                (rank, tensor.shape[0]),\n",
        "                                seed=lseed,\n",
        "                                device=tensor.device,\n",
        "                                dtype=tensor.dtype,\n",
        "                            ) / math.sqrt(rank)\n",
        "\n",
        "                            return left_projection @ tensor\n",
        "                        else:\n",
        "                            right_projection = stable_randn(\n",
        "                                (tensor.shape[-1], rank),\n",
        "                                seed=rseed,\n",
        "                                device=tensor.device,\n",
        "                                dtype=tensor.dtype,\n",
        "                            ) / math.sqrt(rank)\n",
        "                        return tensor @ right_projection\n",
        "\n",
        "                    def _up_proj(seed, rank, shape, ctensor):\n",
        "                        lseed, rseed = split_seed(seed)\n",
        "                        if shape[0] < shape[-1]:\n",
        "                            left_projection = stable_randn(\n",
        "                                (rank, shape[0]),\n",
        "                                seed=lseed,\n",
        "                                device=ctensor.device,\n",
        "                                dtype=ctensor.dtype,\n",
        "                            ) / math.sqrt(rank)\n",
        "                            return left_projection.t() @ ctensor\n",
        "                        else:\n",
        "                            right_projection = stable_randn(\n",
        "                                (shape[-1], rank),\n",
        "                                seed=rseed,\n",
        "                                device=ctensor.device,\n",
        "                                dtype=ctensor.dtype,\n",
        "                            ) / math.sqrt(rank)\n",
        "                            return ctensor @ right_projection.t()\n",
        "\n",
        "                    _current_seed = state[\"seed\"]\n",
        "\n",
        "                    cgrad = _down_proj(seed=_current_seed, rank=group[\"rank\"], tensor=grad)\n",
        "                    print('cgrad =', cgrad.shape)\n",
        "                    # Update biased first moment estimate\n",
        "                    exp_avg.mul_(beta1).add_(cgrad, alpha=1 - beta1)\n",
        "                    print('exp_avg =', exp_avg.shape)\n",
        "                    # Update biased second raw moment estimate\n",
        "                    exp_avg_sq.mul_(beta2).addcmul_(cgrad, cgrad, value=1 - beta2)\n",
        "                    print('exp_avg_sq =', exp_avg_sq.shape)\n",
        "\n",
        "                    # Compute bias-corrected moments\n",
        "                    bias_correction1 = 1 - beta1 ** t\n",
        "                    bias_correction2 = 1 - beta2 ** t\n",
        "                    corrected_avg = exp_avg / bias_correction1\n",
        "                    corrected_avg_sq = exp_avg_sq / bias_correction2\n",
        "\n",
        "                    # Parameter update\n",
        "                    denom = corrected_avg_sq.sqrt().add_(eps)\n",
        "\n",
        "                    print('Decompr corrected_avg =', _up_proj(seed=_current_seed, rank=group[\"rank\"], shape=grad_shape, ctensor=corrected_avg).shape)\n",
        "                    print('Decompr denom =', _up_proj(seed=_current_seed, rank=group[\"rank\"], shape=grad_shape, ctensor=denom).shape)\n",
        "\n",
        "                    p.addcdiv_(_up_proj(seed=_current_seed, rank=group[\"rank\"], shape=grad_shape, ctensor=corrected_avg),\n",
        "                              _up_proj(seed=_current_seed, rank=group[\"rank\"], shape=grad_shape, ctensor=denom),\n",
        "                              value=-lr)\n",
        "\n",
        "                    # Time for a new seed\n",
        "                    if state[\"step\"] % group[\"kappa\"] == 0:\n",
        "                        _next_seed = next_seed(state[\"seed\"])\n",
        "                        print('Setting the new seed =', _next_seed)\n",
        "\n",
        "                        state[\"exp_avg\"].copy_(\n",
        "                            _down_proj(\n",
        "                                seed=_next_seed,\n",
        "                                rank=group[\"rank\"],\n",
        "                                tensor=_up_proj(\n",
        "                                    seed=_current_seed,\n",
        "                                    rank=group[\"rank\"],\n",
        "                                    shape=grad_shape,\n",
        "                                    ctensor=state[\"exp_avg\"]\n",
        "                                    ),\n",
        "                            )\n",
        "                        )\n",
        "\n",
        "                        state[\"exp_avg_sq\"].copy_(\n",
        "                            _down_proj(\n",
        "                                seed=_next_seed,\n",
        "                                rank=group[\"rank\"],\n",
        "                                tensor=_up_proj(\n",
        "                                    seed=_current_seed,\n",
        "                                    rank=group[\"rank\"],\n",
        "                                    shape=grad_shape,\n",
        "                                    ctensor=state[\"exp_avg_sq\"]\n",
        "                                    ),\n",
        "                            )\n",
        "                        )\n",
        "                        state[\"exp_avg\"].to(grad)\n",
        "                        state[\"exp_avg_sq\"].to(grad)\n",
        "\n",
        "                        state[\"seed\"] = _next_seed\n",
        "                        _current_seed = _next_seed\n",
        "\n",
        "                else:\n",
        "                    # Update biased first moment estimate\n",
        "                    exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
        "                    # Update biased second raw moment estimate\n",
        "                    exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
        "\n",
        "                    # Compute bias-corrected moments\n",
        "                    bias_correction1 = 1 - beta1 ** t\n",
        "                    bias_correction2 = 1 - beta2 ** t\n",
        "                    corrected_avg = exp_avg / bias_correction1\n",
        "                    corrected_avg_sq = exp_avg_sq / bias_correction2\n",
        "\n",
        "                    # Parameter update\n",
        "                    denom = corrected_avg_sq.sqrt().add_(eps)\n",
        "                    p.addcdiv_(corrected_avg, denom, value=-lr)\n",
        "\n",
        "        return loss\n"
      ],
      "metadata": {
        "id": "nIg6MMKiSb5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TwoLayerNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(TwoLayerNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)   # First linear layer\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)  # Second linear layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))        # Activation after first layer\n",
        "        x = self.fc2(x)                # Second linear layer\n",
        "        return x     # Softmax for class probabilities\n",
        "\n",
        "model = TwoLayerNet(20, 15, 10)\n",
        "optimizer = FloraAdam(model.parameters(), lr=5e-100, betas=(0.9, 0.999), eps=1e-8, rank = 2, kappa = 2, seed = 0)\n",
        "criterion = torch.nn.MSELoss()\n",
        "\n",
        "x = torch.randn(32, 20)\n",
        "y = torch.randn(32, 10)\n",
        "\n",
        "print(list(model.parameters()), '\\n')\n",
        "\n",
        "# Training step\n",
        "output = model(x)\n",
        "loss = criterion(output, y)\n",
        "\n",
        "optimizer.zero_grad()\n",
        "loss.backward()\n",
        "\n",
        "for epoch in tqdm(range(100), desc=\"Training\"):\n",
        "    model.train()\n",
        "\n",
        "    x = torch.randn(32, 20)\n",
        "    y = torch.randn(32, 10)\n",
        "\n",
        "    output = model(x)\n",
        "\n",
        "    loss = criterion(output, y)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "TLAvKI2rWYJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(model.parameters())"
      ],
      "metadata": {
        "id": "i3un_qs18Y23"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}